{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommend a Key\n",
    "In order to auto recommend wikidata keywords to users, here're two things we need to achieve. \n",
    "\n",
    "First, we'll tokenization the input string and find out which ones can be the keyword for the sentence. We'll introduce a NLP tool develop by the ckiplab of Academic Sinica called ckip-tagger. Within its help, we can do NER to the input stence and hence obtain keywords which are potentially be wikidata keywords.\n",
    "\n",
    "Second, after getting a list of potential words, we'll check if they are wikidata keyword. Here we send request through the wikidata API, to search if the keyword is recorded in wikidata.\n",
    "\n",
    "Finishing the two step works, we'll finally obtain a list of keywords in the input string, and also are wikidata keywords. That result is what we recommend to the users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Before we starting to try this feature, we'll load the input data from Depositar.\n",
    "\n",
    "We previously downloaded metadata of datasets through its API, and store it as file 'datasets.json.' Since we'll only use it as anexample input, here's no need to update this file, and the code for calling the API is not include in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# function definition\n",
    "def get_metadata(data, data_index):\n",
    "    with open(data, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "        title = data[data_index]['title']\n",
    "        notes = data[data_index]['notes']\n",
    "\n",
    "        resources_names = []\n",
    "        resources_desps = []\n",
    "        for item in data[data_index]['resources']:\n",
    "            if 'name' in item:\n",
    "                resources_names.append(item['name'])\n",
    "                resources_desps.append(item['description'])\n",
    "\n",
    "        organization_title = data[data_index]['organization']['title']\n",
    "        organization_desp = data[data_index]['organization']['description']\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Title': [title],\n",
    "        'Notes': [notes],\n",
    "        'Resource Names': [resources_names],\n",
    "        'Resource Descriptions': [resources_desps],\n",
    "        'Organization Title': [organization_title],\n",
    "        'Organization Description': [organization_desp]\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chose one datasets by its index:(from 0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_idx = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets_10.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m(dataset_idx \u001b[39m<\u001b[39m \u001b[39m100\u001b[39m \u001b[39mand\u001b[39;00m dataset_idx \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     df \u001b[39m=\u001b[39m get_metadata(\u001b[39m'\u001b[39;49m\u001b[39mdatasets_10.json\u001b[39;49m\u001b[39m'\u001b[39;49m, dataset_idx)\n\u001b[1;32m      3\u001b[0m     input_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m     \u001b[39m#data_string = df.to_string(index=False, header=False)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mget_metadata\u001b[0;34m(data, data_index)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_metadata\u001b[39m(data, data_index):\n\u001b[0;32m----> 5\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(data, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m         data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n\u001b[1;32m      8\u001b[0m         title \u001b[39m=\u001b[39m data[data_index][\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/2023-sinica-intern/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets_10.json'"
     ]
    }
   ],
   "source": [
    "if(dataset_idx < 100 and dataset_idx > 1):\n",
    "    df = get_metadata('datasets_10.json', dataset_idx)\n",
    "    input_list = []\n",
    "    #data_string = df.to_string(index=False, header=False)\n",
    "    for entity in df:\n",
    "        print(entity, ':', df[entity][0])\n",
    "        input_list.append(df[entity][0])\n",
    "else:\n",
    "    print('input number in the interval from 0 to 99')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: NER task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from transformers import (\n",
    "   BertTokenizerFast,\n",
    "   AutoModelForMaskedLM,\n",
    "   AutoModelForCausalLM,\n",
    "   AutoModelForTokenClassification,\n",
    ")\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModelForTokenClassification.from_pretrained('ckiplab/bert-tiny-chinese-ner')\n",
    "\n",
    "# NLP task model\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "ws_driver  = CkipWordSegmenter(model=\"bert-base\")\n",
    "pos_driver = CkipPosTagger(model=\"bert-base\")\n",
    "ner_driver = CkipNerChunker(model=\"bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 13:12:23.629840: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mckiptagger\u001b[39;00m \u001b[39mimport\u001b[39;00m data_utils, construct_dictionary, WS, POS, NER\n\u001b[0;32m----> 6\u001b[0m ner \u001b[39m=\u001b[39m ner_driver(input_list)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_list' is not defined"
     ]
    }
   ],
   "source": [
    "# NER task\n",
    "import pandas as pd\n",
    "import json\n",
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "\n",
    "ner = ner_driver(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "綠島 : LOC\n",
      "中央研究院 : ORG\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "avoid_class = ['QUANTITY', 'CARDINAL', 'DATE']\n",
    "keyword_map = {}\n",
    "for sentence_ner in ner:\n",
    "   for entity in sentence_ner:\n",
    "      if(entity[1] in avoid_class):\n",
    "        continue\n",
    "      keyword_map[entity[0]] = entity[1]\n",
    "\n",
    "for key, value in keyword_map.items():\n",
    "  print(key, ':', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Searching through Wikidata API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def wiki_search(search_term):\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&format=json&search={search_term}&language=zh\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    # 處理回傳的數據\n",
    "    if \"search\" in data:\n",
    "        for result in data[\"search\"]:\n",
    "            qid = result[\"id\"]\n",
    "            label = result[\"label\"]\n",
    "            description = result.get(\"description\", \"No description available\")\n",
    "            print(f\"QID: {qid}, Label: {label}, Description: {description}\")\n",
    "    else:\n",
    "        print(\"No results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keyword_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m keyword_map:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(item)\n\u001b[1;32m      3\u001b[0m     wiki_search(item)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keyword_map' is not defined"
     ]
    }
   ],
   "source": [
    "for item in keyword_map:\n",
    "    print(item)\n",
    "    wiki_search(item)\n",
    "    print('-------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2023-sinica-intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
