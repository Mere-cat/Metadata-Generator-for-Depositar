{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af805c7",
   "metadata": {},
   "source": [
    "# Recommend Keywords\n",
    "In order to auto recommend wikidata keywords to users, here're two things we need to achieve. \n",
    "\n",
    "First, we'll tokenization the input string and find out which ones can be the keyword for the sentence. We'll introduce a NLP tool develop by the ckiplab of Academic Sinica called ckip-tagger. Within its help, we can do NER to the input stence and hence obtain keywords which are potentially be wikidata keywords.\n",
    "\n",
    "Second, after getting a list of potential words, we'll check if they are wikidata keyword. Here we send request through the wikidata API, to search if the keyword is recorded in wikidata.\n",
    "\n",
    "Finishing the two step works, we'll finally obtain a list of keywords in the input string, and also are wikidata keywords. That result is what we recommend to the users.\n",
    "\n",
    ":::{tip}\n",
    "You can select a specific dataset by its index and see what wikidata we'll recommend to you.\n",
    "First select the \"rocket icon\" on the right-top, then select `Live Code`. After the launching is done, you can run each code cell manually.\n",
    "\n",
    "For the hidden code cell, click `Show code cell source` then click `Run` in each cell section.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1940d142",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Before we start trying this feature, we'll load the input data from Depositar.\n",
    "\n",
    "Previously, we downloaded metadata of datasets from Depositar through its API, randomly selected 10 datasets, and stored them in a file named `example_depositar_data.json` in the `04_data/` directory. Since we'll only use this as an example input, there's no need to update this file, and the code for calling the API is not included in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a4d48",
   "metadata": {},
   "source": [
    "### Obtain metadata from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecebf20e",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# function definition\n",
    "def get_metadata(data, data_index):\n",
    "    with open(data, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "        title = data[data_index]['title']\n",
    "        notes = data[data_index]['notes']\n",
    "\n",
    "        resources_names = []\n",
    "        resources_desps = []\n",
    "        for item in data[data_index]['resources']:\n",
    "            if 'name' in item:\n",
    "                resources_names.append(item['name'])\n",
    "                resources_desps.append(item['description'])\n",
    "\n",
    "        organization_title = data[data_index]['organization']['title']\n",
    "        organization_desp = data[data_index]['organization']['description']\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Title': [title],\n",
    "        'Notes': [notes],\n",
    "        'Resource Names': [resources_names],\n",
    "        'Resource Descriptions': [resources_desps],\n",
    "        'Organization Title': [organization_title],\n",
    "        'Organization Description': [organization_desp]\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9a834",
   "metadata": {},
   "source": [
    "We can chose one datasets by its index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1635ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_idx = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a069df",
   "metadata": {},
   "source": [
    "✨ You can change the index to see the result of different dataset. (from 0 to 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d9bc3",
   "metadata": {},
   "source": [
    "### Show dataset content\n",
    "Below displays the information of our selected dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc737d6f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : 林邊排水水質自然淨化工程-林邊排水水質自然淨化處理場域規劃設計\n",
      "Notes : \n",
      "Resource Names : ['']\n",
      "Resource Descriptions : ['']\n",
      "Organization Title : 「全國水環境改善計畫」108-109年度屏東縣政府水環境改善輔導顧問團\n",
      "Organization Description : \n"
     ]
    }
   ],
   "source": [
    "if(dataset_idx < 100 and dataset_idx > 1):\n",
    "    df = get_metadata('../../assets/example_depositar_data.json', dataset_idx)\n",
    "    input_list = []\n",
    "    for entity in df:\n",
    "        print(entity, ':', df[entity][0])\n",
    "        input_list.append(df[entity][0])\n",
    "else:\n",
    "    print('input number in the interval from 0 to 99')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a20b09",
   "metadata": {},
   "source": [
    "## Step 1: NER task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d52275",
   "metadata": {},
   "source": [
    "### Import Models\n",
    "Here we import the transformer models, and do the NER to our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f41700",
   "metadata": {
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# NLP task model\n",
    "from ckip_transformers.nlp import CkipNerChunker\n",
    "ner_driver = CkipNerChunker(model=\"bert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d10fb",
   "metadata": {},
   "source": [
    "### NER task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ccec2aa",
   "metadata": {
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenization:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenization: 100%|██████████| 6/6 [00:00<00:00, 33916.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# NER task\n",
    "ner = ner_driver(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af53e4",
   "metadata": {},
   "source": [
    "### Output\n",
    "Below is the output list of the NER result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10668088",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "林邊 : GPE\n",
      "屏東縣政府 : ORG\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "avoid_class = ['QUANTITY', 'CARDINAL', 'DATE', 'ORDINAL']\n",
    "keyword_map = {}\n",
    "for sentence_ner in ner:\n",
    "   for entity in sentence_ner:\n",
    "      if(entity[1] in avoid_class):\n",
    "        continue\n",
    "      keyword_map[entity[0]] = entity[1]\n",
    "\n",
    "for key, value in keyword_map.items():\n",
    "  print(key, ':', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882eb04",
   "metadata": {},
   "source": [
    "## Step 2: Searching through Wikidata API\n",
    "After searching each potential word obtained in previous step, now we are going to check if each word is a wikidata keyword."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfd01c",
   "metadata": {},
   "source": [
    "### Request Wikidata API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebd526e0",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def wiki_search(search_term):\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&format=json&search={search_term}&language=zh\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    # organize the response\n",
    "    if \"search\" in data:\n",
    "        for result in data[\"search\"]:\n",
    "            qid = result[\"id\"]\n",
    "            label = result[\"label\"]\n",
    "            description = result.get(\"description\", \"No description available\")\n",
    "            print(f\"QID: {qid}, Label: {label}, Description: {description}\")\n",
    "    else:\n",
    "        print(\"No results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b140c",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7ca3a",
   "metadata": {},
   "source": [
    "Here is the output of searching result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040eeb5a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "林邊\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QID: Q708239, Label: Linbian, Description: rural township of Taiwan\n",
      "QID: Q7564094, Label: Woodside, Description: neighborhood in Queens, New York City, United States\n",
      "QID: Q6550302, Label: Linbian River, Description: river in Taiwan\n",
      "QID: Q1033565, Label: Linbian Station, Description: railway station\n",
      "QID: Q17026596, Label: Linbian Interchange, Description: No description available\n",
      "QID: Q11107072, Label: Woodside, Description: Residential building in Hong Kong\n",
      "QID: Q97025818, Label: Linbian Village, Description: Village in Linbian, Pingtung County, Taiwan\n",
      "-------------------------------------------\n",
      "屏東縣政府\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QID: Q11042707, Label: Pingtung County Government, Description: executive branch of Pingtung County, Taiwan\n",
      "QID: Q107359613, Label: Department of Public Works, Pingtung County Government, Description: No description available\n",
      "QID: Q115624611, Label: 屏東縣政府消防局, Description: No description available\n",
      "QID: Q115619758, Label: 屏東縣政府警察局, Description: No description available\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for item in keyword_map:\n",
    "    print(item)\n",
    "    wiki_search(item)\n",
    "    print('-------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.1"
   }
  },
  "kernelspec": {
   "display_name": "2023-sinica-intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "source_map": [
   12,
   30,
   37,
   41,
   74,
   78,
   80,
   84,
   89,
   100,
   104,
   109,
   115,
   119,
   124,
   129,
   143,
   148,
   152,
   172,
   176,
   180
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}