{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008a2862",
   "metadata": {},
   "source": [
    "# Recommend a Key\n",
    "In order to auto recommend wikidata keywords to users, here're two things we need to achieve. \n",
    "\n",
    "First, we'll tokenization the input string and find out which ones can be the keyword for the sentence. We'll introduce a NLP tool develop by the ckiplab of Academic Sinica called ckip-tagger. Within its help, we can do NER to the input stence and hence obtain keywords which are potentially be wikidata keywords.\n",
    "\n",
    "Second, after getting a list of potential words, we'll check if they are wikidata keyword. Here we send request through the wikidata API, to search if the keyword is recorded in wikidata.\n",
    "\n",
    "Finishing the two step works, we'll finally obtain a list of keywords in the input string, and also are wikidata keywords. That result is what we recommend to the users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5135477a",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Before we starting to try this feature, we'll load the input data from Depositar.\n",
    "\n",
    "We previously downloaded metadata of datasets through its API, and store it as file 'datasets.json.' Since we'll only use it as an example input, here's no need to update this file, and the code for calling the API is not include in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f22741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# function definition\n",
    "def get_metadata(data, data_index):\n",
    "    with open(data, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "        title = data[data_index]['title']\n",
    "        notes = data[data_index]['notes']\n",
    "\n",
    "        resources_names = []\n",
    "        resources_desps = []\n",
    "        for item in data[data_index]['resources']:\n",
    "            if 'name' in item:\n",
    "                resources_names.append(item['name'])\n",
    "                resources_desps.append(item['description'])\n",
    "\n",
    "        organization_title = data[data_index]['organization']['title']\n",
    "        organization_desp = data[data_index]['organization']['description']\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Title': [title],\n",
    "        'Notes': [notes],\n",
    "        'Resource Names': [resources_names],\n",
    "        'Resource Descriptions': [resources_desps],\n",
    "        'Organization Title': [organization_title],\n",
    "        'Organization Description': [organization_desp]\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3f68c",
   "metadata": {},
   "source": [
    "We can chose one datasets by its index:(from 0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce64b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_idx = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b797042",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets_10.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(dataset_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dataset_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mget_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets_10.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     input_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#data_string = df.to_string(index=False, header=False)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m, in \u001b[0;36mget_metadata\u001b[0;34m(data, data_index)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_metadata\u001b[39m(data, data_index):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m      8\u001b[0m         title \u001b[38;5;241m=\u001b[39m data[data_index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.8.17/x64/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets_10.json'"
     ]
    }
   ],
   "source": [
    "if(dataset_idx < 100 and dataset_idx > 1):\n",
    "    df = get_metadata('datasets_10.json', dataset_idx)\n",
    "    input_list = []\n",
    "    #data_string = df.to_string(index=False, header=False)\n",
    "    for entity in df:\n",
    "        print(entity, ':', df[entity][0])\n",
    "        input_list.append(df[entity][0])\n",
    "else:\n",
    "    print('input number in the interval from 0 to 99')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9caac",
   "metadata": {},
   "source": [
    "## Step 1: NER task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a73872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from transformers import (\n",
    "   BertTokenizerFast,\n",
    "   AutoModelForMaskedLM,\n",
    "   AutoModelForCausalLM,\n",
    "   AutoModelForTokenClassification,\n",
    ")\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModelForTokenClassification.from_pretrained('ckiplab/bert-tiny-chinese-ner')\n",
    "\n",
    "# NLP task model\n",
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "ws_driver  = CkipWordSegmenter(model=\"bert-base\")\n",
    "pos_driver = CkipPosTagger(model=\"bert-base\")\n",
    "ner_driver = CkipNerChunker(model=\"bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3754a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER task\n",
    "import pandas as pd\n",
    "import json\n",
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "\n",
    "ner = ner_driver(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "avoid_class = ['QUANTITY', 'CARDINAL', 'DATE']\n",
    "keyword_map = {}\n",
    "for sentence_ner in ner:\n",
    "   for entity in sentence_ner:\n",
    "      if(entity[1] in avoid_class):\n",
    "        continue\n",
    "      keyword_map[entity[0]] = entity[1]\n",
    "\n",
    "for key, value in keyword_map.items():\n",
    "  print(key, ':', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ea399",
   "metadata": {},
   "source": [
    "## Step 2: Searching through Wikidata API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77707497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def wiki_search(search_term):\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&format=json&search={search_term}&language=zh\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    # 處理回傳的數據\n",
    "    if \"search\" in data:\n",
    "        for result in data[\"search\"]:\n",
    "            qid = result[\"id\"]\n",
    "            label = result[\"label\"]\n",
    "            description = result.get(\"description\", \"No description available\")\n",
    "            print(f\"QID: {qid}, Label: {label}, Description: {description}\")\n",
    "    else:\n",
    "        print(\"No results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02806d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in keyword_map:\n",
    "    print(item)\n",
    "    wiki_search(item)\n",
    "    print('-------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.1"
   }
  },
  "kernelspec": {
   "display_name": "2023-sinica-intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "source_map": [
   12,
   23,
   30,
   61,
   65,
   69,
   79,
   83,
   101,
   110,
   122,
   126,
   146
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}