{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c75db620",
   "metadata": {},
   "source": [
    "# Recommend Keywords\n",
    "In order to auto recommend wikidata keywords to users, here're two things we need to achieve. \n",
    "\n",
    "First, we'll tokenization the input string and find out which ones can be the keyword for the sentence. We'll introduce a NLP tool develop by the ckiplab of Academic Sinica called ckip-tagger. Within its help, we can do NER to the input stence and hence obtain keywords which are potentially be wikidata keywords.\n",
    "\n",
    "Second, after getting a list of potential words, we'll check if they are wikidata keyword. Here we send request through the wikidata API, to search if the keyword is recorded in wikidata.\n",
    "\n",
    "Finishing the two step works, we'll finally obtain a list of keywords in the input string, and also are wikidata keywords. That result is what we recommend to the users.\n",
    "\n",
    ":::{tip}\n",
    "You can select a specific dataset by its index and see what wikidata we'll recommend to you.\n",
    "First select the \"rocket icon\" on the right-top, then select `Live Code`. After the launching is done, you can run each code cell manually.\n",
    "\n",
    "For the hidden code cell, click `Show code cell source` then click `Run` in each cell section.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e020e2",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Before we start trying this feature, we'll load the input data from Depositar.\n",
    "\n",
    "Previously, we downloaded metadata of datasets from Depositar through its API, randomly selected 10 datasets, and stored them in a file named `example_depositar_data.json` in the `04_data/` directory. Since we'll only use this as an example input, there's no need to update this file, and the code for calling the API is not included in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d59bd03",
   "metadata": {},
   "source": [
    "### Obtain metadata from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189073e1",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# function definition\n",
    "def get_metadata(data_path, data_index):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "        data = pd.read_json(data_path)\n",
    "\n",
    "        title = data.loc[data_index, 'title']\n",
    "        notes = data.loc[data_index, 'notes']\n",
    "\n",
    "        resources_names = []\n",
    "        resources_desps = []\n",
    "        for item in data.loc[data_index, 'resources']:\n",
    "            if 'name' in item:\n",
    "                resources_names.append(item['name'])\n",
    "                resources_desps.append(item['description'])\n",
    "\n",
    "        organization_title = data.loc[data_index, 'organization']['title']\n",
    "        organization_desp = data.loc[data_index, 'organization']['description']\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'Title': [title],\n",
    "            'Notes': [notes],\n",
    "            'Resource Names': [resources_names],\n",
    "            'Resource Descriptions': [resources_desps],\n",
    "            'Organization Title': [organization_title],\n",
    "            'Organization Description': [organization_desp]\n",
    "        })\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9d636",
   "metadata": {},
   "source": [
    "We can chose one datasets by its index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7666930",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_idx = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af675aaa",
   "metadata": {},
   "source": [
    "âœ¨ You can change the index to see the result of different dataset. (from 0 to 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e8d3d0",
   "metadata": {},
   "source": [
    "### Show dataset content\n",
    "Below displays the information of our selected dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf637af",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'warnings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(dataset_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dataset_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://mere-cat.github.io/Metadata-Generator-for-Depositar/assets/example_depositar_data.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mget_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     input_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m df:\n",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m, in \u001b[0;36mget_metadata\u001b[0;34m(data_path, data_index)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_metadata\u001b[39m(data_path, data_index):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mwarnings\u001b[49m\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m      7\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m         data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(data_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'warnings' is not defined"
     ]
    }
   ],
   "source": [
    "if(dataset_idx < 100 and dataset_idx > 1):\n",
    "    data_path = 'https://mere-cat.github.io/Metadata-Generator-for-Depositar/assets/example_depositar_data.json'\n",
    "    df = get_metadata(data_path, dataset_idx)\n",
    "    input_list = []\n",
    "    for entity in df:\n",
    "        print(entity, ':', df[entity][0])\n",
    "        input_list.append(df[entity][0])\n",
    "else:\n",
    "    print('input number in the interval from 0 to 99')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0445ed",
   "metadata": {},
   "source": [
    "## Step 1: NER task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272152b",
   "metadata": {},
   "source": [
    "### Import Models\n",
    "Here we import the transformer models, and do the NER to our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1788626",
   "metadata": {
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# NLP task model\n",
    "from ckip_transformers.nlp import CkipNerChunker\n",
    "ner_driver = CkipNerChunker(model=\"bert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de24cc49",
   "metadata": {},
   "source": [
    "### NER task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673d17f",
   "metadata": {
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# NER task\n",
    "ner = ner_driver(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4038aa8",
   "metadata": {},
   "source": [
    "### Output\n",
    "Below is the output list of the NER result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aeff5b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Show results\n",
    "avoid_class = ['QUANTITY', 'CARDINAL', 'DATE', 'ORDINAL']\n",
    "keyword_map = {}\n",
    "for sentence_ner in ner:\n",
    "   for entity in sentence_ner:\n",
    "      if(entity[1] in avoid_class):\n",
    "        continue\n",
    "      keyword_map[entity[0]] = entity[1]\n",
    "\n",
    "for key, value in keyword_map.items():\n",
    "  print(key, ':', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d07497",
   "metadata": {},
   "source": [
    "## Step 2: Searching through Wikidata API\n",
    "After searching each potential word obtained in previous step, now we are going to check if each word is a wikidata keyword."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d4cd9f",
   "metadata": {},
   "source": [
    "### Request Wikidata API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f9ab4",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def wiki_search(search_term):\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&format=json&search={search_term}&language=zh\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    # organize the response\n",
    "    if \"search\" in data:\n",
    "        for result in data[\"search\"]:\n",
    "            qid = result[\"id\"]\n",
    "            label = result[\"label\"]\n",
    "            description = result.get(\"description\", \"No description available\")\n",
    "            print(f\"QID: {qid}, Label: {label}, Description: {description}\")\n",
    "    else:\n",
    "        print(\"No results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570b78d9",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b05db96",
   "metadata": {},
   "source": [
    "Here is the output of searching result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7764c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "for item in keyword_map:\n",
    "    print(item)\n",
    "    wiki_search(item)\n",
    "    print('-------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.1"
   }
  },
  "kernelspec": {
   "display_name": "2023-sinica-intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "source_map": [
   12,
   30,
   37,
   41,
   76,
   80,
   82,
   86,
   91,
   103,
   107,
   112,
   118,
   122,
   127,
   132,
   146,
   151,
   155,
   175,
   179,
   183
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}