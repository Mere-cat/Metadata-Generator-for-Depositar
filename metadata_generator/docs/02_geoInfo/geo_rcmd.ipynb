{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommend GeoInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To automatically recommend locations along with their geographic information to users, we need to accomplish two key objectives.\n",
    "\n",
    "To begin with, we will tokenize the input string to identify potential keywords for the given input. To achieve this, we will utilize a Natural Language Processing (NLP) tool developed by the ckiplab at Academic Sinica, called `ckiplab/bert-base-chinese-ner`. With the assistance of this tool, we can perform Named Entity Recognition (NER) on the input sentence, thereby extracting keywords that could potentially match Wikidata keywords.\n",
    "\n",
    "Subsequently, once we have compiled a list of potential words, our next step involves verifying whether these words correspond to actual locations across the globe. For this purpose, we will make use of the Nominatim API to retrieve search results for the list of potential keywords.\n",
    "\n",
    "Upon successfully completing these two steps, we will have a comprehensive compilation of locations and their associated geographic information. To provide users with an interactive preview of OpenStreetMap (OSM), we have implemented [folium](http://python-visualization.github.io/folium/index.html). This enables users to select a location from the aforementioned list, input its geographic details, and visualize it on the map.\n",
    "\n",
    ":::{tip}\n",
    "You can choose a particular dataset by its index and explore the geographic information recommendations provided. To proceed, follow these steps:\n",
    "1. Click on the \"rocket icon\" located in the top-right corner.\n",
    "2. Select the option labeled `Live Code` from the menu.\n",
    "3. Once the environment is launched, you'll be able to manually execute **each** code cell.\n",
    "\n",
    "For any hidden code cells, simply click on `Show code cell source` and subsequently click `run` within each respective cell section.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Before we start trying this feature, we'll load the input data from Depositar.\n",
    "\n",
    "Previously, we downloaded metadata of datasets from Depositar through its API, randomly selected 10 datasets, and stored them in a file named `example_depositar_data.json` in the `assets/` directory. Since we'll only use this as an example input, there's no need to update this file, and the code for calling the API is not included in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain metadata from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# function definition\n",
    "def get_metadata(data_path, data_index):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "        data = pd.read_json(data_path)\n",
    "\n",
    "        title = data.loc[data_index, 'title']\n",
    "        notes = data.loc[data_index, 'notes']\n",
    "\n",
    "        resources_names = []\n",
    "        resources_desps = []\n",
    "        for item in data.loc[data_index, 'resources']:\n",
    "            if 'name' in item:\n",
    "                resources_names.append(item['name'])\n",
    "                resources_desps.append(item['description'])\n",
    "\n",
    "        organization_title = data.loc[data_index, 'organization']['title']\n",
    "        organization_desp = data.loc[data_index, 'organization']['description']\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'Title': [title],\n",
    "            'Notes': [notes],\n",
    "            'Resource Names': [resources_names],\n",
    "            'Resource Descriptions': [resources_desps],\n",
    "            'Organization Title': [organization_title],\n",
    "            'Organization Description': [organization_desp]\n",
    "        })\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chose one datasets by its index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_idx = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ú® You can change the index to see the result of different dataset. (from 0 to 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show dataset content\n",
    "Below displays the information of our selected dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : ÊûóÈÇäÊéíÊ∞¥Ê∞¥Ë≥™Ëá™ÁÑ∂Ê∑®ÂåñÂ∑•Á®ã-ÊûóÈÇäÊéíÊ∞¥Ê∞¥Ë≥™Ëá™ÁÑ∂Ê∑®ÂåñËôïÁêÜÂ†¥ÂüüË¶èÂäÉË®≠Ë®à\n",
      "Notes : \n",
      "Resource Names : ['']\n",
      "Resource Descriptions : ['']\n",
      "Organization Title : „ÄåÂÖ®ÂúãÊ∞¥Áí∞Â¢ÉÊîπÂñÑË®àÁï´„Äç108-109Âπ¥Â∫¶Â±èÊù±Á∏£ÊîøÂ∫úÊ∞¥Áí∞Â¢ÉÊîπÂñÑËºîÂ∞éÈ°ßÂïèÂúò\n",
      "Organization Description : \n"
     ]
    }
   ],
   "source": [
    "if(dataset_idx < 100 and dataset_idx > 1):\n",
    "    data_path = 'https://mere-cat.github.io/Metadata-Generator-for-Depositar/assets/example_depositar_data.json'\n",
    "    df = get_metadata(data_path, dataset_idx)\n",
    "    input_list = []\n",
    "    for entity in df:\n",
    "        print(entity, ':', df[entity][0])\n",
    "        input_list.append(df[entity][0])\n",
    "else:\n",
    "    print('input number in the interval from 0 to 99')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: NER task\n",
    "\n",
    "### Import Models\n",
    "Here we import the transformer models, and do the NER to our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# NLP task model\n",
    "from ckip_transformers.nlp import CkipNerChunker\n",
    "ner_driver = CkipNerChunker(model=\"bert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 25165.82it/s]\n",
      "Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# NER task\n",
    "ner = ner_driver(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "Below is the output list of the NER result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊûóÈÇä : GPE\n",
      "Â±èÊù±Á∏£ÊîøÂ∫ú : ORG\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "avoid_class = ['QUANTITY', 'CARDINAL', 'DATE', 'ORDINAL']\n",
    "keyword_map = {}\n",
    "for sentence_ner in ner:\n",
    "   for entity in sentence_ner:\n",
    "      if(entity[1] in avoid_class):\n",
    "        continue\n",
    "      keyword_map[entity[0]] = entity[1]\n",
    "\n",
    "for key, value in keyword_map.items():\n",
    "  print(key, ':', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Searching through OSM Nominatim API\n",
    "After searching each potential word obtained in previous step, now we are going to check if each word is a wikidata keyword.\n",
    "\n",
    "### Requeest Nominatim API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_osm_place(query):\n",
    "    base_url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"format\": \"json\",\n",
    "        \"polygon_geojson\": \"1\",  # Request GeoJSON polygons\n",
    "        \"limit\": 7\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSM Search Result\n",
    "Now, we'll obtain a list of possible location for each named entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSM result for ÊûóÈÇä is:\n",
      "üìç ÊûóËæπ, Á§ºÂéø, ÈôáÂçóÂ∏Ç, ÁîòËÇÉÁúÅ, ‰∏≠ÂõΩ\n",
      "{\"type\": \"Point\", \"coordinates\": [105.247, 34.1612]}\n",
      "üìç ÊûóËæπ, ÂçéÂù™Âéø, ‰∏ΩÊ±üÂ∏Ç, ‰∫ëÂçóÁúÅ, ‰∏≠ÂõΩ\n",
      "{\"type\": \"Point\", \"coordinates\": [101.2901069, 26.7634965]}\n",
      "üìç ÊûóÈÇä, ÈªÉÂüîÊùë, ÁÉàÂ∂ºÈÑâ, ÈáëÈñÄÁ∏£, 894, Ëá∫ÁÅ£\n",
      "{\"type\": \"Point\", \"coordinates\": [118.2514395, 24.4465422]}\n",
      "-------------------------------------------\n",
      "OSM result for Â±èÊù±Á∏£ÊîøÂ∫ú is:\n",
      "üìç Â±èÊù±Á∏£ÊîøÂ∫ú, 527, Ëá™Áî±Ë∑Ø, ÂãùÂà©Èáå, Â±èÊù±Â∏Ç, Â±èÊù±Á∏£, 90001, Ëá∫ÁÅ£\n",
      "{\"type\": \"Point\", \"coordinates\": [120.4879096, 22.6829927]}\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for item in keyword_map:\n",
    "    result = search_osm_place(item)\n",
    "    if result:\n",
    "        print(f\"OSM result for {item} is:\")\n",
    "        for place in result:\n",
    "            print(\"üìç\", place[\"display_name\"])\n",
    "            print(str(place[\"geojson\"]).replace(\"'\", \"\\\"\"))\n",
    "        print('-------------------------------------------')\n",
    "    else:\n",
    "            print(\"No geoInfo provided.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input geoJSON\n",
    "Select one of the location above, copy the geoJSON and paste to the below cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoInfo = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the Location in OSM\n",
    "Now, we can see the location you previously chosen displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please paste the geoJSON in the 'geoInfo' string.\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "\n",
    "center_coords = [25.041415686746607, 121.61472689731077]  # Sinica\n",
    "m = folium.Map(location=center_coords, zoom_start=12)\n",
    "\n",
    "if(len(geoInfo) == 0):\n",
    "    print(\"please paste the geoJSON in the 'geoInfo' string.\")\n",
    "else:\n",
    "    geojson = eval(geoInfo)\n",
    "    folium.GeoJson(geojson).add_to(m)\n",
    "    m.fit_bounds(m.get_bounds())\n",
    "    display(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2023-sinica-intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
