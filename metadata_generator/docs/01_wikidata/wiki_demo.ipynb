{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Demo\n",
    "\n",
    "You can try this feature with your own data in this page. \n",
    "\n",
    ":::{tip}\n",
    "You can input the information of your own dataset to try this feature.\n",
    "\n",
    "First select the \"rocket icon\" on the right-top, then select `Live Code`. After the launching is done, you can run each code cell manually.\n",
    "\n",
    "For the hidden code cell, click `Show code cell source` then click `Run` in each cell section.\n",
    ":::\n",
    "\n",
    "Here're the information you may input:\n",
    "* `title`: the title of your dataset\n",
    "* `description`: description of your dataset\n",
    "* `resource_names`: file names in your dataset\n",
    "* `resource_descriptions`: the description of files in your dataset\n",
    "* `organization_title`: the tilte of the affiliatedorganization \n",
    "* `organization_description`: the description of the affiliatedorganization \n",
    "\n",
    "```{warning} Notice\n",
    "At least one of the fields should be completed. Leaving all of them empty is not permissible.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definations\n",
    "Just expend it and click `Run`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "remove-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Packages Import ============================================================\n",
    "import requests\n",
    "\n",
    "# NLP task model\n",
    "from ckip_transformers.nlp import CkipNerChunker\n",
    "ner_driver = CkipNerChunker(model=\"bert-base\")\n",
    "\n",
    "# Function Definstion =========================================================\n",
    "def wiki_search(search_term):\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&format=json&search={search_term}&language=zh\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    # organize the response\n",
    "    if \"search\" in data:\n",
    "        for result in data[\"search\"]:\n",
    "            qid = result[\"id\"]\n",
    "            label = result[\"label\"]\n",
    "            description = result.get(\"description\", \"No description available\")\n",
    "            print(f\"QID: {qid}, Label: {label}, Description: {description}\")\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "\n",
    "def make_keyword_map(input_list):\n",
    "    # NER task\n",
    "    ner = ner_driver(input_list)\n",
    "\n",
    "    # Build keyword_map to store potential words\n",
    "    avoid_class = ['QUANTITY', 'CARDINAL', 'DATE', 'ORDINAL']\n",
    "    keyword_map = {}\n",
    "    for sentence_ner in ner:\n",
    "        for entity in sentence_ner:\n",
    "            if(entity[1] in avoid_class):\n",
    "                continue\n",
    "        keyword_map[entity[0]] = entity[1]\n",
    "    \n",
    "    return keyword_map\n",
    "\n",
    "def gen_keyword(title, description, resource_names, resource_descriptions, organization_title, organization_description):\n",
    "    input_list = [title, description, resource_names, resource_descriptions, organization_title, organization_description]\n",
    "\n",
    "    if all(not item for item in input_list):\n",
    "        return -1\n",
    "    else:\n",
    "       keyword_map = make_keyword_map(input_list)\n",
    "       return keyword_map\n",
    "\n",
    "def output(result):\n",
    "    if(result == -1):\n",
    "        print(\"At least one of the fields should be completed. Leaving all of them empty is not permissible.\")\n",
    "        return\n",
    "    else:\n",
    "        for item in result:\n",
    "            print(item)\n",
    "            wiki_search(item)\n",
    "            print('-------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "âœ¨ You can type information of your own dataset here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "title = ''\n",
    "description = ''\n",
    "resource_names = []\n",
    "resource_descriptions = []\n",
    "organization_title = \"\"\n",
    "organization_description = \"\"\n",
    "\n",
    "result = gen_keyword(title, description, resource_names, resource_descriptions, organization_title, organization_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "Below's our recomnned wikidata keyword(s) for your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2023-sinica-intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
